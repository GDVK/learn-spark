{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8252bbda",
   "metadata": {},
   "source": [
    "A **schema** is information about the data contained in a DataFrame. Specifically, the number of columns, column names, column data type, and whether the column can contain NULLs. Without a schema, a DataFrame would be a group of disorganized things. The schema gives the DataFrame structure and meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450cb01",
   "metadata": {},
   "source": [
    "**Without a schema, a DataFrame would be a group of disorganized things. The schema gives the DataFrame structure and meaning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2a761",
   "metadata": {},
   "source": [
    "In the previous section we introduced the createDataFrame() method. In PySpark, this method looks like:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "675a1db7",
   "metadata": {},
   "source": [
    "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815d9d3",
   "metadata": {},
   "source": [
    "After the required data parameter the first optional parameter is schema. The most useful options for the schema parameter include: None (or not included), a list of column names, or a StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914ae72",
   "metadata": {},
   "source": [
    "After the required data parameter the first optional parameter is schema. The most useful options for the schema parameter include: None (or not included), a list of column names, or a StructType.\n",
    "\n",
    "If schema is None or left out, then Spark will try to infer the column names and the column types from the data. If schema is a list of column names, then Spark will add the column names in the order specified and will try to infer the column types from the data. In both of these cases Spark uses the number of rows specified in the second optional parameter, **samplingRatio**, to infer the schema from the data. If not included or given None, then only the top row is used to infer the schema.\n",
    "\n",
    "To illustrate, say we had some data with a variable named computer_sales with columns \"product_code\", \"computer_name\", and \"sales\". The following illustrates all the options the createDataFrame() method can handle in PySpark.\n",
    "\n",
    "The following code is used when only the data parameter is provided or the schema is set to None or left blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8167cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\sparkhome'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1692c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55b87342",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Vamsi_App\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f03b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "computer_sales = [\n",
    "    {'product_code': 'A123', 'computer_name': 'Laptop A', 'sales': 100},\n",
    "    {'product_code': 'B456', 'computer_name': 'Desktop B', 'sales': 150}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3e68c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(computer_sales) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a67ab26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(computer_sales, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd6655f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-----+\n",
      "|computer_name|product_code|sales|\n",
      "+-------------+------------+-----+\n",
      "|     Laptop A|        A123|  100|\n",
      "|    Desktop B|        B456|  150|\n",
      "+-------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b7476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-----+\n",
      "|computer_name|product_code|sales|\n",
      "+-------------+------------+-----+\n",
      "|     Laptop A|        A123|  100|\n",
      "|    Desktop B|        B456|  150|\n",
      "+-------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395321b9",
   "metadata": {},
   "source": [
    "Both DataFrames are equivalent.\n",
    "\n",
    "The following is used when the data parameter is specified along with a Python list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23168e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'product_code': 'A123', 'computer_name': 'Laptop A', 'sales': 100},\n",
       " {'product_code': 'B456', 'computer_name': 'Desktop B', 'sales': 150}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computer_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cf7dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.createDataFrame(computer_sales, ['product_code', 'computer_name', 'sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc4882df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----+\n",
      "|product_code|computer_name|sales|\n",
      "+------------+-------------+-----+\n",
      "|    Laptop A|         A123|  100|\n",
      "|   Desktop B|         B456|  150|\n",
      "+------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b601ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.createDataFrame(computer_sales, [\"product_code\", \"computer_name\", \"sales\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51872cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----+\n",
      "|product_code|computer_name|sales|\n",
      "+------------+-------------+-----+\n",
      "|    Laptop A|         A123|  100|\n",
      "|   Desktop B|         B456|  150|\n",
      "+------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97c42efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = spark.createDataFrame(computer_sales, [\"product_code\", \"computer_name\", \"sales\"], len(computer_sales))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a7589",
   "metadata": {},
   "source": [
    "The following is used to infer the schema from every row in the DataFrame. len() is a Python function that returns an integer of the number of values in a list. Since the number of values in the list computer_sales equals the number of rows in the DataFrame, the samplingRatio parameter will evaluate every row in the DataFrame to infer the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee49f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----+\n",
      "|product_code|computer_name|sales|\n",
      "+------------+-------------+-----+\n",
      "|    Laptop A|         A123|  100|\n",
      "|   Desktop B|         B456|  150|\n",
      "+------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bced342",
   "metadata": {},
   "source": [
    "#### Exercise 6: Creating a DataFrame in PySpark with only named columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d639501",
   "metadata": {},
   "source": [
    "Create a nested list called home_computers as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "753f4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_computers = [[\"Honeywell\", \"Honeywell 316#Kitchen Computer\", \"DDP 16 Minicomputer\", 1969], [\"Apple Computer\", \"Apple II series\", \"6502\", 1977], [\"Bally Consumer Products\", \"Bally Astrocade\", \"Z80\", 1977]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "214b623f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Honeywell', 'Honeywell 316#Kitchen Computer', 'DDP 16 Minicomputer', 1969],\n",
       " ['Apple Computer', 'Apple II series', '6502', 1977],\n",
       " ['Bally Consumer Products', 'Bally Astrocade', 'Z80', 1977]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_computers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62241d46",
   "metadata": {},
   "source": [
    "Create a DataFrame but this time the column names of the DataFrame are given explicitly as a list in the second parameter as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e87bc2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "computers_df = spark.createDataFrame(home_computers, [\"Manufacturer\", \"Model\", \"Processor\", \"Year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e55a0",
   "metadata": {},
   "source": [
    "Since the third parameter **samplingRatio** is not included, Spark uses the first row of data to infer the data types of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ebb0be",
   "metadata": {},
   "source": [
    "Show the contents of the DataFrame and display the schema with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "437d4873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+----+\n",
      "|        Manufacturer|               Model|          Processor|Year|\n",
      "+--------------------+--------------------+-------------------+----+\n",
      "|           Honeywell|Honeywell 316#Kit...|DDP 16 Minicomputer|1969|\n",
      "|      Apple Computer|     Apple II series|               6502|1977|\n",
      "|Bally Consumer Pr...|     Bally Astrocade|                Z80|1977|\n",
      "+--------------------+--------------------+-------------------+----+\n",
      "\n",
      "root\n",
      " |-- Manufacturer: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Processor: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "computers_df.show()\n",
    "computers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6804b3f",
   "metadata": {},
   "source": [
    "Columns names make DataFrames exceptionally useful. The PySpark API makes adding columns names to a DataFrame very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07f1fd",
   "metadata": {},
   "source": [
    "### Schemas, StructTypes, and StructFields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadf287",
   "metadata": {},
   "source": [
    "The most rigid and defined option for schema is the **StructType**. It is important to note that the schema of a DataFrame a StructType. **If a DataFrame is created without column names and Spark infers the data types based upon the data, a StructType is still created in the background by Spark.**\n",
    "\n",
    "A manually created PySpark DataFrame, like the following example, still has a StructType schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3d92447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+----+\n",
      "|                  _1|                  _2|                 _3|  _4|\n",
      "+--------------------+--------------------+-------------------+----+\n",
      "|           Honeywell|Honeywell 316#Kit...|DDP 16 Minicomputer|1969|\n",
      "|      Apple Computer|     Apple II series|               6502|1977|\n",
      "|Bally Consumer Pr...|     Bally Astrocade|                Z80|1977|\n",
      "+--------------------+--------------------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "computers_df_1 = spark.createDataFrame(home_computers)\n",
    "computers_df_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc8b074",
   "metadata": {},
   "source": [
    "The schema can be displayed in PySpark by calling the schema method on a DataFrame like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f935d467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('_1', StringType(), True), StructField('_2', StringType(), True), StructField('_3', StringType(), True), StructField('_4', LongType(), True)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computers_df_1.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5ab0bf",
   "metadata": {},
   "source": [
    "To recap, **the schema of a DataFrame is stored as a StructType object. The StructType object consists of a list of StructFields**. The StructFields are the information about the columns of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdd13067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "  \n",
    "schema = StructType([\n",
    "  StructField(\"Manufacturer\", StringType(), True),\n",
    "  StructField(\"Model\", StringType(), True),\n",
    "  StructField(\"Processor\", StringType(), True),\n",
    "  StructField(\"Year\", LongType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616bc6c",
   "metadata": {},
   "source": [
    "***It is important to note that the schema of a DataFrame is a StructType***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f26d3",
   "metadata": {},
   "source": [
    "StructFields are objects that correspond to each column of the DataFrame and are constructed with the name, data type, and a boolean value of whether the column can contain NULLs. The second parameter of a StructFieldis the columns data type: string, integer, decimal, datetime, and so on. To use data types in Spark the types module must be called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1628108",
   "metadata": {},
   "source": [
    "To use data types in Spark the types module must be called. Imports in Scala and Python are code that is not built-in the main module. For example, \n",
    "\n",
    "DataFrames are part of the main code class. But ancillary things like data types and functions are not and must be imported to be used in your file. The following code is the Scala import for all of the data types:\n",
    "\n",
    "The following code is the Python import for all of the data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0605a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d4e10",
   "metadata": {},
   "source": [
    "**Note:** StructType and StructField are actually Spark data types themselves. They are included in the preceding data imports that import all the members of the data types class. To import StructType and StructField individually use the following code for PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e8b32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf38ee",
   "metadata": {},
   "source": [
    "#### Exercise 7: Creating a DataFrame in PySpark with a Defined Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98cc483",
   "metadata": {},
   "source": [
    "Import all the PySpark data types at once (that include both StructType and StructField) and make a nested list of data with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58f928cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    " \n",
    "customer_list = [[111, \"Jim\", 45.51], [112, \"Fred\", 87.3], [113, \"Jennifer\", 313.69], [114, \"Lauren\", 28.78]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41ff03",
   "metadata": {},
   "source": [
    "Construct the schema using the StructType and StructField. First make a StructType which holds a Python list as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5b0be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\",LongType(),True),\n",
    "    StructField(\"first_name\",StringType(),True),\n",
    "    StructField(\"avg_shopping_cart\",DoubleType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af700fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.createDataFrame(customer_list,schema=customer_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0bf0f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------------+\n",
      "|customer_id|first_name|avg_shopping_cart|\n",
      "+-----------+----------+-----------------+\n",
      "|        111|       Jim|            45.51|\n",
      "|        112|      Fred|             87.3|\n",
      "|        113|  Jennifer|           313.69|\n",
      "|        114|    Lauren|            28.78|\n",
      "+-----------+----------+-----------------+\n",
      "\n",
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- avg_shopping_cart: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()\n",
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c665585",
   "metadata": {},
   "source": [
    "Spark schemas are the structure or the scaffolding of a DataFrame. Just like a building would collapse without structure, so too would a DataFrame. Without structure Spark wouldn’t be able to scale to trillions and trillions of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d55156",
   "metadata": {},
   "source": [
    "**Spark schemas are the structure or the scaffolding of a DataFrame**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58440e77",
   "metadata": {},
   "source": [
    "#### 1) The add() Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c01ffa",
   "metadata": {},
   "source": [
    "The add() method can be used interchangeably and in addition to the StructFields objects on a StructType. The add() method takes the same parameters as the StructField object. The following schemas are all equivalent representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8c4e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "  \n",
    "schema1 = StructType([\n",
    "  StructField(\"id_column\", LongType(), True),\n",
    "  StructField(\"product_desc\", StringType(), True)\n",
    "])\n",
    "  \n",
    "schema2 = StructType().add(\"id_column\", LongType(), True).add(\"product_desc\", StringType(), True)\n",
    "  \n",
    "schema3 = StructType().add(StructField(\"id_column\", LongType(), True)).add(StructField(\"product_desc\", StringType(), True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16640f7a",
   "metadata": {},
   "source": [
    "We can confirm that the two schemas are equivalent by comparing if the schema variables are equal to each other and printing the results in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d24a101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(schema1 == schema2) \n",
    "print(schema1 == schema3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d544726",
   "metadata": {},
   "source": [
    "#### Exercise 8: Using the add() Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7cb175",
   "metadata": {},
   "source": [
    "Create a schema of a StructType named sales_schema that has two columns. The first column “user_id” is a long data type and cannot be nullable. The second column “product_item” is a string data type and can be nullable. Following is the code for PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2390bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, LongType\n",
    "\n",
    "sales_schema = StructType([\n",
    "  StructField(\"user_id\", LongType(), False),\n",
    "  StructField(\"product_item\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a67078",
   "metadata": {},
   "source": [
    "Create a StructField called sales_field that has a column name of \"total_sales\" with a long data type that can be nullable. Following is the code for PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc5ada26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_field = StructField(\"total_sales\",LongType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "467639fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "another_schema = sales_schema.add(sales_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0693a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('user_id', LongType(), False), StructField('product_item', StringType(), True), StructField('total_sales', LongType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(another_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c509a",
   "metadata": {},
   "source": [
    "#### Use the add() method when adding columns to a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1149a",
   "metadata": {},
   "source": [
    "You can use the schema method on a DataFrame in conjunction with the add() method to add new fields to the schema of an already existing DataFrame. In Spark, a DataFrame's schema is a StructType. In the preceding exercise we manually specified the schema as StructType. Spark has a shortcut: the schema method. The method schema can be called on an existing DataFrame to return its schema, that is a StructType. So in Spark Scala or PySpark you would call some_df.schema to output the StructType schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f311f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('customer_id', LongType(), True), StructField('first_name', StringType(), True), StructField('avg_shopping_cart', DoubleType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(customer_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a16eb5",
   "metadata": {},
   "source": [
    "So, with the schema method you don't have to manually create the StructType to add a new column. Just call the schema method on the DataFrame and then use the add method to add a column as a StructField. Following is the code for PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76fe0879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('customer_id', LongType(), True), StructField('first_name', StringType(), True), StructField('avg_shopping_cart', DoubleType(), True), StructField('new_column', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "final_schema = customer_df.schema.add(StructField(\"new_column\", StringType(), True)) \n",
    "print(final_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420239b",
   "metadata": {},
   "source": [
    "#### Return column names from a schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ba24b",
   "metadata": {},
   "source": [
    "Use the fieldNames method on a StructType to return a list or array of the column names. This is an easy way to return the column names of a DataFrame. The fieldNames method can be called on a StructType or after the schema method on a DataFrame. The only difference between Spark Scala and PySpark is that PySpark requires trailing parenthesis () and Spark Scala omits the parenthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece2beb",
   "metadata": {},
   "source": [
    "In PySpark call the fieldNames() method on a schema and on the DataFrame to return the column names of the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "285a0937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_id', 'first_name', 'avg_shopping_cart', 'new_column']\n",
      "['customer_id', 'first_name', 'avg_shopping_cart', 'new_column']\n"
     ]
    }
   ],
   "source": [
    "print( customer_df.schema.fieldNames() ) \n",
    "print( customer_schema.fieldNames() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce8444",
   "metadata": {},
   "source": [
    "### Nested Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f11d712",
   "metadata": {},
   "source": [
    "So far, we have dealt with flat and orderly DataFrame schema. But Spark supports nested columns where a column can contain more sets of data. Suppose we had a data set that looked like the following Python dictionary or JSON object:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf95bd9",
   "metadata": {},
   "source": [
    "{\"id\":101,\"name\":\"Jim\",\"orders\":[{\"id\":1,\"price\":45.99,\"userid\":101},{\"id\":2,\"price\":17.35,\"userid\":101}]},{\"id\":102,\"name\":\"Christina\",\"orders\":[{\"id\":3,\"price\":245.86,\"userid\":102}]},{\"id\":103,\"name\":\"Steve\",\"orders\":[{\"id\":4,\"price\":7.45,\"userid\":103},{\"id\":5,\"price\":8.63,\"userid\":103}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55752612",
   "metadata": {},
   "source": [
    "This data set would be the result of some imaginary sales tables that was joined to an orders table. We will look at joining DataFrames together in Chapter 3, SQL with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e72d8",
   "metadata": {},
   "source": [
    "It is difficult to see from the nested dictionary but there are three columns: id, name, and orders. But orders is special, because it is a list of lists. In Python we can directly use this data by wrapping it in brackets as mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92e0ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_sales_data = [{\"id\":101,\"name\":\"Jim\",\"orders\":[{\"id\":1,\"price\":45.99,\"userid\":101},{\"id\":2,\"price\":17.35,\"userid\":101}]},{\"id\":102,\"name\":\"Christina\",\"orders\":[{\"id\":3,\"price\":245.86,\"userid\":102}]},{\"id\":103,\"name\":\"Steve\",\"orders\":[{\"id\":4,\"price\":7.45,\"userid\":103},{\"id\":5,\"price\":8.63,\"userid\":103}]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "705981d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 101,\n",
       "  'name': 'Jim',\n",
       "  'orders': [{'id': 1, 'price': 45.99, 'userid': 101},\n",
       "   {'id': 2, 'price': 17.35, 'userid': 101}]},\n",
       " {'id': 102,\n",
       "  'name': 'Christina',\n",
       "  'orders': [{'id': 3, 'price': 245.86, 'userid': 102}]},\n",
       " {'id': 103,\n",
       "  'name': 'Steve',\n",
       "  'orders': [{'id': 4, 'price': 7.45, 'userid': 103},\n",
       "   {'id': 5, 'price': 8.63, 'userid': 103}]}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_sales_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573c544",
   "metadata": {},
   "source": [
    "If we used this list and made a DataFrame without specifying a schema, the output would not be very usable or readable. The following PySpark code uses the preceding nested JSON data to make a Spark DataFrame. The DataFrame and schema is displayed to demonstrate what can happen when you make a DataFrame with nested data without a schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "110625bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------------------------------------------------------------------------+\n",
      "|id |name     |orders                                                                            |\n",
      "+---+---------+----------------------------------------------------------------------------------+\n",
      "|101|Jim      |[{id -> 1, userid -> 101, price -> null}, {id -> 2, userid -> 101, price -> null}]|\n",
      "|102|Christina|[{id -> 3, userid -> 102, price -> null}]                                         |\n",
      "|103|Steve    |[{id -> 4, userid -> 103, price -> null}, {id -> 5, userid -> 103, price -> null}]|\n",
      "+---+---------+----------------------------------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- orders: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: long (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df = spark.createDataFrame(nested_sales_data) \n",
    "sales_df.show(20, False) \n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac67c331",
   "metadata": {},
   "source": [
    "The output is not readable or user friendly with the “-> \" characters and Spark is trying to make a map of the data. Let's add a schema to tell Spark exactly how we want to structure the DataFrame. The following PySpark code demonstrates the results of nested data when using a schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2adaf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "  \n",
    "orders_schema = [\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"price\", DoubleType(), True),\n",
    "  StructField(\"userid\", IntegerType(), True)\n",
    "]\n",
    "  \n",
    "sales_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"name\", StringType(), True),\n",
    "  StructField(\"orders\", ArrayType(StructType(orders_schema)), True) #ArrayType() applied in StructField\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b66c05",
   "metadata": {},
   "source": [
    "Here we called the **order_schema** inside the **sales_schema**. This shows how versatile schemas can be and how easy it is in Spark to construct complex schemas. Now let's make a DataFrame that is readable and well structured as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1f5c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_df = spark.createDataFrame(nested_sales_data, sales_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b94dc2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------------------------+\n",
      "|id |name     |orders                            |\n",
      "+---+---------+----------------------------------+\n",
      "|101|Jim      |[{1, 45.99, 101}, {2, 17.35, 101}]|\n",
      "|102|Christina|[{3, 245.86, 102}]                |\n",
      "|103|Steve    |[{4, 7.45, 103}, {5, 8.63, 103}]  |\n",
      "+---+---------+----------------------------------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- orders: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: integer (nullable = true)\n",
      " |    |    |-- price: double (nullable = true)\n",
      " |    |    |-- userid: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nested_df.show(20, False)\n",
    "nested_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4437c276",
   "metadata": {},
   "source": [
    "In the next section we will move on from manually created DataFrames to creating DataFrames from files stored in Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451f45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
