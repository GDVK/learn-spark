{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82781ab",
   "metadata": {},
   "source": [
    "### Creating a DataFrame in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3170687",
   "metadata": {},
   "source": [
    "The main way to create DataFrames in Spark is to use the createDataFrame() method. This method is called on the SparkSession to create DataFrames. Since Spark 2.0, the SparkSession is the main way to interact with all of Sparks' many capabilities. The SparkSession creates and exposes the SparkConf, SparkContext, and SQLContext to the entire application. Normally the SparkSession is created with a variable named \"spark\" but any name can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72400ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vamsi\n"
     ]
    }
   ],
   "source": [
    "print(\"Vamsi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b9843d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\sparkhome'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28b0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bedae5c",
   "metadata": {},
   "source": [
    "A basic SparkSession in PySpark looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a32b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Vamsi_App\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427bb9c",
   "metadata": {},
   "source": [
    "The createDataFrame() method in PySpark has a structure like:\n",
    "\n",
    "###### createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5be97",
   "metadata": {},
   "source": [
    "### List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b939252",
   "metadata": {},
   "source": [
    "#### Exercise 1: Creating a DataFrame in PySpark from a Nested List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec31e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_list = [[1, \"MapReduce\"], [2, \"YARN\"], [3, \"Hive\"], [4, \"Pig\"], [5, \"Spark\"], [6, \"Zookeeper\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1316b475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'MapReduce'],\n",
       " [2, 'YARN'],\n",
       " [3, 'Hive'],\n",
       " [4, 'Pig'],\n",
       " [5, 'Spark'],\n",
       " [6, 'Zookeeper']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hadoop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d0ec9",
   "metadata": {},
   "source": [
    "To create the DataFrame named hadoop_df we use the SparkSession variable spark (that we created) and call the createDataFrame() method passing only the nested list with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab55af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_df = spark.createDataFrame(hadoop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf80fc9",
   "metadata": {},
   "source": [
    "Finally display the contents of the DataFrame using hadoop_df.show() and display the schema of the DataFrame in a tree structure using hadoop_df.printSchema() as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6a51be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| _1|       _2|\n",
      "+---+---------+\n",
      "|  1|MapReduce|\n",
      "|  2|     YARN|\n",
      "|  3|     Hive|\n",
      "|  4|      Pig|\n",
      "|  5|    Spark|\n",
      "|  6|Zookeeper|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hadoop_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4912522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hadoop_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d0cf8",
   "metadata": {},
   "source": [
    "You have now created your first Spark DataFrame. In this exercise the DataFrame only has six rows. But a Spark DataFrame can scale infinitely to contain 100 trillion rows and beyond. This the power of Spark.\n",
    "\n",
    "In the output did you notice anything that stood out? There are actually two things to note:\n",
    "\n",
    "The column names were _1 and _2. ***This is because no column names were supplied when creating the DataFrame***. Spark didn't know what to call the columns, so _1 and _2 correspond to its column number going from left to right.\n",
    "The output of the printSchema() method correctly inferred the data type of each column. Spark figured out the first column was of data type long, which is similar to an integer, and that the second column was a string. When the printSchema() method is called on a DataFrame the output displays the schema in a tree format. The tree format displays the column hierarchy, column names, column data type, and whether the column is nullable. ***The printSchema() method has no parameters.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0741c1",
   "metadata": {},
   "source": [
    "To display the contents of the DataFrame we call the method show() on the newly created DataFrame. The show() method looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00438b49",
   "metadata": {},
   "source": [
    "show(n=20, truncate=True)\n",
    "\n",
    "The show() method defaults to displaying the top twenty rows and also truncates each cell to the first twenty characters. To display more or less than the top twenty rows set the first parameter to any integer. To include all the characters of the cells, set the second parameter to False.\n",
    "\n",
    "Displaying the contents of Spark DataFrames in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec54ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "address_data = [[\"Bob\", \"1348 Central Park Avenue\"], [\"Nicole\", \"734 Southwest 46th Street\"], [\"Jordan\", \"3786 Ocean City Drive\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c3a18a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bob', '1348 Central Park Avenue'],\n",
       " ['Nicole', '734 Southwest 46th Street'],\n",
       " ['Jordan', '3786 Ocean City Drive']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2584708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "address_df = spark.createDataFrame(address_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e33536fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|    _1|                  _2|\n",
      "+------+--------------------+\n",
      "|   Bob|1348 Central Park...|\n",
      "|Nicole|734 Southwest 46t...|\n",
      "|Jordan|3786 Ocean City D...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "address_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49152e",
   "metadata": {},
   "source": [
    "Since the method defaults to the displaying only the first twenty characters of each cell, the content is truncated.\n",
    "\n",
    "To display all the characters for each cell set the second parameter to False and to limit the output to the first two rows, set the first parameter to 2, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7701609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------+\n",
      "|_1    |_2                       |\n",
      "+------+-------------------------+\n",
      "|Bob   |1348 Central Park Avenue |\n",
      "|Nicole|734 Southwest 46th Street|\n",
      "+------+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "address_df.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d16449",
   "metadata": {},
   "source": [
    "**Note:** The second parameter, truncate, can also take integers. If set to an integer, it will display the number of characters equal to the integer for each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5322c",
   "metadata": {},
   "source": [
    "### tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44d9631",
   "metadata": {},
   "source": [
    "In Python, a tuple is similar to a list except it is wrapped in parentheses instead of square brackets and is not changeable (immutable). Other than that, lists and tuples are the same. A nested tuple is a tuple inside another tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1153b24",
   "metadata": {},
   "source": [
    "#### Exercise 2: Creating a DataFrame in PySpark from a nested tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aeb804",
   "metadata": {},
   "source": [
    "Create a nested tuple called programming_languages with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f37503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_languages = ((1, \"Java\", \"Scalable\"), (2, \"C\", \"Portable\"), (3, \"Python\", \"Big Data, ML, AI, Robotics\"), (4, \"JavaScript\", \"Web Browsers\"), (5, \"Ruby\", \"Web Apps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c629e1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 'Java', 'Scalable'),\n",
       " (2, 'C', 'Portable'),\n",
       " (3, 'Python', 'Big Data, ML, AI, Robotics'),\n",
       " (4, 'JavaScript', 'Web Browsers'),\n",
       " (5, 'Ruby', 'Web Apps'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "programming_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36c1e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_lang_df = spark.createDataFrame(programming_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59394468",
   "metadata": {},
   "source": [
    "Display the five rows and set the truncate parameter to False so the entire contents of the cells will be shown. Also print the schema of the DataFrame with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69264a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------------+\n",
      "|_1 |_2        |_3                        |\n",
      "+---+----------+--------------------------+\n",
      "|1  |Java      |Scalable                  |\n",
      "|2  |C         |Portable                  |\n",
      "|3  |Python    |Big Data, ML, AI, Robotics|\n",
      "|4  |JavaScript|Web Browsers              |\n",
      "|5  |Ruby      |Web Apps                  |\n",
      "+---+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prog_lang_df.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac07eb92",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a276d62",
   "metadata": {},
   "source": [
    "In Python, a dictionary is a key-value pair wrapped in curly braces. A dictionary is similar to a list, in that it is mutable, can increase or decrease in size, and be nested. Each data element in a dictionary has a key and a value. **To create a DataFrame out of a dictionary all that is required is to wrap it in a list.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739a32ed",
   "metadata": {},
   "source": [
    "#### Exercise 3: Creating a DataFrame in PySpark from a list of dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1c3bf",
   "metadata": {},
   "source": [
    "Create a list of dictionaries called top_mobile_phones. Inside the list make three comma separated dictionaries each with keys of \"Manufacturer\", \"Model\", \"Year\", \"Million_Units\" as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6da37568",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mobile_phones = [{\"Manufacturer\": \"Nokia\", \"Model\": \"1100\", \"Year\": 2003, \"Million_Units\": 250}, {\"Manufacturer\": \"Nokia\", \"Model\": \"1110\", \"Year\": 2005, \"Million_Units\": 250}, {\"Manufacturer\": \"Apple\", \"Model\": \"iPhone 6 & 6+\", \"Year\": 2014, \"Million_Units\": 222}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbec687e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Manufacturer': 'Nokia',\n",
       "  'Model': '1100',\n",
       "  'Year': 2003,\n",
       "  'Million_Units': 250},\n",
       " {'Manufacturer': 'Nokia',\n",
       "  'Model': '1110',\n",
       "  'Year': 2005,\n",
       "  'Million_Units': 250},\n",
       " {'Manufacturer': 'Apple',\n",
       "  'Model': 'iPhone 6 & 6+',\n",
       "  'Year': 2014,\n",
       "  'Million_Units': 222}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_mobile_phones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6511a47",
   "metadata": {},
   "source": [
    "Create a DataFrame called mobile_phones_df from the dictionary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14df0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_phones_df = spark.createDataFrame(top_mobile_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "488e2d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------------+----+\n",
      "|Manufacturer|Million_Units|        Model|Year|\n",
      "+------------+-------------+-------------+----+\n",
      "|       Nokia|          250|         1100|2003|\n",
      "|       Nokia|          250|         1110|2005|\n",
      "|       Apple|          222|iPhone 6 & 6+|2014|\n",
      "+------------+-------------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mobile_phones_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6d2cc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Manufacturer: string (nullable = true)\n",
      " |-- Million_Units: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mobile_phones_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c0eca",
   "metadata": {},
   "source": [
    "Notice that **we didn’t supply the column names to the DataFrame but they still appear. That is because dictionaries have “keys” and these keys make up the columns of the DataFrame**. Likewise, the dictionary “values” are the cells in the DataFrame. So, by using dictionaries, Spark can display the DataFrame column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f420211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
